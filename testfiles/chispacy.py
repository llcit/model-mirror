import spacy

nlp = spacy.load('zh_core_web_trf')

doc = nlp(u"\u5728\u6570\u5b66\u6559\u80b2\u9886\u57df\u4e2d\uff0c\u4eba\u4eec\u5e38\u5e38\u9519\u8bef\u5730\u8ba4\u4e3a\u901a\u8fc7\u53e3\u5934\u89e3\u91ca\u6570\u5b66\u7b56\u7565\u53ef\u4ee5\u4f7f\u5b66\u751f\u7406\u89e3\u3002\u7136\u800c\uff0c\u514b\u6797\u548c\u8d1d\u4f0a-\u5a01\u5ec9\u59c6\u65af\u8ba4\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u4e0d\u80fd\u6307\u671b\u5b66\u751f\u4ec5\u51ed\u88ab\u544a\u77e5\u5c31\u80fd\u7406\u89e3\u3002\u4ed6\u4eec\u5f3a\u8c03\u63d0\u4f9b\u5b66\u751f\u5145\u8db3\u7684\u65f6\u95f4\u548c\u673a\u4f1a\u6765\u7406\u89e3\u548c\u53ef\u89c6\u5316\u6570\u5b57\u5173\u7cfb\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u53e3\u5934\u89e3\u91ca\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u79cd\u7406\u89e3\uff0c\u7814\u7a76\u4eba\u5458\u4e3b\u5f20\u5e7f\u6cdb\u4f7f\u7528\u89c6\u89c9\u7b56\u7565\u3002\u4e00\u79cd\u6709\u8da3\u7684\u6d3b\u52a8\u662f\u4f7f\u7528\u201c\u5feb\u901f\u67e5\u770b\u5361\u7247\u201d\u3002\u8fd9\u4e9b\u5361\u7247\u7b80\u8981\u663e\u793a\u4e86\u4e00\u4e9b\u70b9\u6216\u719f\u6089\u7269\u4f53\uff08\u5982\u9e21\u86cb\uff09\u7684\u56fe\u50cf\uff0c\u4fc3\u4f7f\u5b66\u751f\u786e\u5b9a\u6570\u91cf\u548c\u89c2\u5bdf\u65b9\u5f0f\u3002\u4f8b\u5982\uff0c\u4ed6\u4eec\u662f\u770b\u5230\u4e86\u56db\u7ec4\u4e24\u4e2a\u8fd8\u662f\u4e24\u7ec4\u56db\u4e2a\uff1f\u901a\u8fc7\u7528\u70b9\u6216\u64cd\u4f5c\u7269\u4f53\u6765\u89c6\u89c9\u5316\u8868\u793a\u6570\u5b57\uff0c\u5b66\u751f\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6570\u5b66\u7b56\u7565\u80cc\u540e\u7684\u539f\u7406\u548c\u7406\u7531\u3002\u53e6\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\u662f\u201cSplat!\u201d\uff0c\u8fd9\u662f\u4fc4\u52d2\u5188\u5dde\u7684\u6570\u5b66\u6559\u7ec3\u53f2\u8482\u592b\u00b7\u5a01\u4f2f\u5c3c\u521b\u9020\u7684\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5e7b\u706f\u7247\uff0c\u5b66\u751f\u88ab\u8981\u6c42\u6570\u4e00\u6570\u4ed6\u4eec\u770b\u5230\u7684\u70b9\u7684\u6570\u91cf\uff0c\u7136\u540e\u662f\u4e00\u4e2a\u6709\u4e9b\u70b9\u88ab\u6d82\u62b9\u7684\u5e7b\u706f\u7247\u3002\u8fd9\u9f13\u52b1\u5b66\u751f\u5236\u5b9a\u81ea\u5df1\u7684\u7b56\u7565\uff0c\u65e0\u8bba\u662f\u5411\u4e0a\u8fd8\u662f\u5411\u4e0b\u8ba1\u6570\uff0c\u8fd8\u662f\u5229\u7528\u58f0\u97f3\u6216\u624b\u6307\u3002")
assert doc.has_annotation("SENT_START")
base = ""
sentcounter = 1
#every word w token.pos >2 gap astrix before+after gap
for sent in doc.sents:
    counter = 1
    nounFound = False
    for token in sent:
        if counter%2 == 0 and nounFound is False and len(token.shape_) > 1 and sentcounter != 1:
            ttoken = str(token)
            newword, trash = divmod(len(ttoken), 2)
            ttoken = ttoken[:newword] + "*" + ttoken[newword:] + "*" + "(" + token.pos_ + ")"
            #nounFound = True
        elif token.pos_ == 'PUNCT':
            ttoken = str(token) + " "
        else:
            ttoken = str(token)
        counter = counter + 1
        base = base + ttoken
    sentcounter = sentcounter + 1
        
base = base.lstrip(' ')
print(doc.text + '\n\n')
print(base)

#print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
#        token.shape_, token.is_alpha, token.is_stop)
#    print('\n\n\n')
#+ "(" + token.pos_ + ")"
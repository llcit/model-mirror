{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeb Lakey\n",
    "\n",
    "Dr. Richard Medina\n",
    "\n",
    "Model in the Mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the documentation/thought process during the Model in the Mirror project running from July 20 - August 25 2023. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First File: apitest.py\n",
    "Used to get used to certain parts of the openai library and to test the API call. Worked correctly. References trainingdata.jsonl and validationfile.jsonl, which were empty JSON Lines files that the fine-tuning training draws from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.21.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "import openai\n",
    "import json\n",
    "\n",
    "APIKEY = 'samplekey'\n",
    "openai.api_key = APIKEY\n",
    "trainingfile = \"trainingdata.jsonl\"\n",
    "validationfile = \"validationdata.jsonl\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",\n",
    "  prompt=\"Who won the super bowl in 2015?\",\n",
    "  max_tokens=30\n",
    ")\n",
    "print(response)\n",
    "#Response: The New England Patriots won the Super Bowl in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part was generating a bank of questions and answers to be used as a control group to compare to the responses of the fine tuned model we will be creating later. The topic that was came up with was Astronomy, and the difficulty levels used were divided into Elementary, Intermediate, and Advanced. Responses were generated below using GPT 3.5.\n",
    "\n",
    "5 different question structures were utilized per reading level. This was to test if the way in which the question was asked impacted prompt quality. It didn't, with the responses generated even being the exact same in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Elementary\n",
    "\n",
    "<br>TC1\n",
    "<br>Q: \"write a 25-word, elementary level passage about astronomy in english\" \n",
    "<br>A: \"In astronomy, we study space and everything in it. We learn about stars, planets, and the moon. It's like exploring a vast cosmic playground!\"\n",
    "<br>Word Count: 24\n",
    "<br>TC2\n",
    "<br>Q: \"write an elementary level english passage about astronomy that is 25 words long\"\n",
    "<br>A: \"Astronomy is all about space. We learn about stars, planets, and the moon. It's exciting to discover the secrets of the universe!\"\n",
    "<br>Word Count: 22\n",
    "<br>TC3\n",
    "<br>Q: \"write a 25-word, elementary level passage in english about astronomy\"\n",
    "<br>A: \"Astronomy is about space and stars. We study planets and the moon. It's fun to learn about the universe and its wonders!\"\n",
    "<br>Word Count: 22\n",
    "<br>TC4\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words LANGUAGE COMPLEXITY: Elementary TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"Astronomy is about space and stars. We study planets and the moon. It's fun to learn about the universe and its wonders!\"\n",
    "<br>Word Count: 22\n",
    "<br>TC5\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words READING LEVEL: Elementary TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"Explore the sky's mysteries in astronomy. Stars, planets, and the moon amaze us. Join the cosmic journey, learning the wonders above.\"\n",
    "<br>Word Count: 21\n",
    "\n",
    "2: Intermediate\n",
    "\n",
    "<br>TC1\n",
    "<br>Q: \"write a 25-word, intermediate level passage about astronomy in english\"\n",
    "<br>A: \"Astronomy is the study of celestial objects like stars, planets, and galaxies. It explores the universe's origins, evolution, and phenomena like black holes and supernovae.\"\n",
    "<br>Word Count: 25\n",
    "<br>TC2\n",
    "<br>Q: \"write an intermediate level english passage about astronomy that is 25 words long\"\n",
    "<br>A: \"Astronomy delves into celestial wonders - stars, planets, galaxies. We explore the universe's mysteries: dark matter, cosmic evolution, and the search for extraterrestrial life.\"\n",
    "<br>Word Count: 23\n",
    "<br>TC3\n",
    "<br>Q: \"write a 25-word, intermediate level passage in english about astronomy\"\n",
    "<br>A: \"Astronomy uncovers celestial marvels - stars, planets, galaxies. We explore cosmic phenomena: black holes, dark matter, and the quest for extraterrestrial intelligence.\"\n",
    "<br>Word Count: 21\n",
    "<br>TC4\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words LANGUAGE COMPLEXITY: Intermediate TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"Astronomy, a profound cosmic pursuit, explores celestial marvels: stars, planets, galaxies. Unravel enigmasâ€”black holes, dark matter, and the search for life beyond Earth. Embark on astronomical wonders!\"\n",
    "<br>Word Count: 28\n",
    "<br>A: \"Astronomy, the study of celestial objects and phenomena, unveils the wonders of the universe. From stars to galaxies, it ignites curiosity and expands human understanding.\"\n",
    "<br>Word Count: 25\n",
    "<br>TC5\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words READING LEVEL: Intermediate TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"Astronomy unlocks the cosmos. Discover stars, planets, and galaxies. Unravel mysteries like black holes and dark matter. The universe awaits exploration!\"\n",
    "<br>Word Count: 21\n",
    "\n",
    "3: Advanced\n",
    "\n",
    "<br>TC1\n",
    "<br>Q: \"write a 25-word, advanced level passage about astronomy in english\"\n",
    "<br>A: \"Astronomy, the venerable science of cosmic exploration, delves into the intricate workings of celestial entities, unearthing cosmic origins and probing the enigmas of the universe.\"\n",
    "<br>Word Count: 25\n",
    "<br>TC2\n",
    "<br>Q: \"write an advanced level english passage about astronomy that is 25 words long\"\n",
    "<br>A: \"Astronomy, an intricate cosmic pursuit, unravels celestial origins, explores galaxies, and illuminates the universe's enigmatic wonders, expanding our cosmic consciousness.\"\n",
    "<br>Word Count: 20\n",
    "<br>TC3\n",
    "<br>Q: \"write a 25-word, advanced level passage in english about astronomy\"\n",
    "<br>A: \"Astronomy, the profound study of celestial phenomena, unveils cosmic mysteries, such as black holes and dark matter, reshaping our perception of the cosmos.\"\n",
    "<br>Word Count: 23\n",
    "<br>TC4\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words LANGUAGE COMPLEXITY: Advanced TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"In the realm of astronomy, celestial symphonies play out as galaxies collide, birthing quasars that illuminate the cosmic tapestry, revealing its enigmatic dance of dark matter and energy.\"\n",
    "<br>Word Count: 28\n",
    "<br>TC5\n",
    "<br>Q: \"write a passage with these parameters: LENGTH: 25 words READING LEVEL: Advanced TOPIC: Astronomy LANGUAGE: English\"\n",
    "<br>A: \"Amid the vast expanse of the cosmos, astronomy beckons explorers to decipher the secrets of celestial realms, igniting wonder and enlightening humanity's cosmic journey.\"\n",
    "<br>\n",
    "Word Count: 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it was at this stage when we realized that the models available for fine tuning were very far away from actually being able to be used for text generation. It was at this point that we re-evaluated our focus and turned towards using GPT 3.5 to generate prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMuPDF was used to scrape a pdf file. The contents of a couple pages of this pdf was then fed into GPT to create 3 100-word passages from the contents it was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.24.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "\n",
    "import fitz\n",
    "import openai\n",
    "\n",
    "APIKEY = 'samplekey'\n",
    "openai.api_key = APIKEY\n",
    "\n",
    "doc = fitz.open(\"sample.pdf\")\n",
    "\n",
    "instruction = 'You are a language teacher tasked with developing 100 word passages for the purpose of language instruction. These passages will be used to test language learners with their corresponding literacy levels. '\n",
    "text = 'QUERY: write 3 100-word summaries in english about this exerpt: '\n",
    "\n",
    "for page in doc.pages(2,6,1):\n",
    "    extract = page.get_text()\n",
    "    text += extract\n",
    "doc.close()\n",
    "\n",
    "def returnPrompts(sinput, cinput):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": sinput},\n",
    "        {\"role\": \"user\", \"content\": cinput}],\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "    )\n",
    "    reply = response['choices'][0]['message']['content']\n",
    "    return reply\n",
    "\n",
    "reply = returnPrompts(instruction, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked well, but the passages generated still need to be separated into difficulty levels. NOTE: Tried to change wording to summaries, passages worked better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I received definitions of each of the language levels from another intern at the CLT, Ashley Nichols. They are shown here:\n",
    "\n",
    "Beginner:\n",
    "\"Beginner means that the language used has short sentences, \n",
    "simple grammar patterns, and uses vocabulary words of high \n",
    "frequency.\";\n",
    "\n",
    "Intermediate:\n",
    "\"Intermediate means that the language used has somewhat complex \n",
    "grammar, more infrequent vocabulary, \n",
    "and longer sentences, but does not contain extremely \n",
    "complex grammar, jargon, or complex sentences.\";\n",
    "\n",
    "Advanced:\n",
    "\"Advanced means that the language used has complex grammar, \n",
    "contains infrequent vocabulary words and jargon, and has \n",
    "lengthy sentences.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded acceptable results for the current stage of the project when tested on the online chatbot version of gpt3.5. I then moved on to creating another system prompt + function definition for the simplification/modification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.25.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "intro = 'You are a writer tasked with modifying a set of passages to a set of instructions. These are the instructions: '\n",
    "beginnerInstruction = 'Simplify the following passages to beginner level, which means the language used has short sentences, simple grammar patterns, and uses vocabulary words of high frequency.'\n",
    "intermediateInstruction = 'Modify the following passages to intermediate level, which means that the language used has somewhat complex grammar, more infrequent vocabulary, and longer sentences, but does not contain extremely complex grammar, jargon, or complex sentences.'\n",
    "advancedInstruction = 'Modify the following passages to advanced level, which means that the language used has complex grammar, contains infrequent vocabulary words and jargon, and has lengthy sentences.'\n",
    "\n",
    "#have some form of user input here for choice of instruction type\n",
    "\n",
    "userInput = intro + beginnerInstruction\n",
    "\n",
    "def simplifyReplies(sysinput, cinput):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": sysinput},\n",
    "        {\"role\": \"user\", \"content\": cinput}],\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "    )\n",
    "    passage = response['choices'][0]['message']['content']\n",
    "    return passage\n",
    "\n",
    "finalPassages = simplifyReplies(userInput, reply) #reply is the reply that the first api call produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded 3 passages that were at an acceptable level of difficulty at the beginner level. Next was to write a simple bit of code to write the user prompts and the corresponding content generated to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FULLRUNTEST.txt', 'a') as f:\n",
    "    f.write('\\n\\nSYSTEM PROMPT: ' + instruction + '\\n\\n')\n",
    "    f.write('QUERY: write 3 200-word passages in english about this exerpt: \\n\\n')\n",
    "    f.write('TEXT INPUT: ' + text + '\\n\\n')\n",
    "    f.write('RESPONSE: ' + reply + '\\n\\n')\n",
    "    f.write('QUERY 2: ' + userInput + '\\n\\n')\n",
    "    f.write('Final Passsages' + finalPassages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also did some cleaning up of the initial code and added some exception handling in the pdf reading section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.25.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "text = 'QUERY: write 3 120-word passages in english about this exerpt: '\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(\"sample.pdf\")\n",
    "except SystemError:\n",
    "        print(\"error opening pdf\")\n",
    "else:\n",
    "    for page in doc.pages(2,doc.page_count,1):\n",
    "        if(len(text) < 10000):\n",
    "            extract = page.get_text()\n",
    "            text += extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was creating a database for the prompts to be stored in after processing. This was done using a simple dictionary with keys such as Culture, Sports, and Astronomy. The value corresponding to these keys is a Python list that holds the prompts. The dictionary is stored in a json and accessed using load/dump. This was tested in a separate file before being implimented in the main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.25.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "import json\n",
    "\n",
    "with open(\"promptdatabase.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "data[\"Culture\"].append(\"New Prompt\")\n",
    "data[\"Math\"].append(\"New Prompt\")\n",
    "data[\"Science\"].append(\"New Prompt\")\n",
    "data[\"Language\"].append(\"New Prompt\")\n",
    "\n",
    "with open(\"promptdatabase.json\", \"w\") as write_file:\n",
    "    json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorization is fairly simple. I set up the prompt for the simplifyReplies function to include a delimiter between the passages, separated them using split(), and placed the now separate passages into an array. To do the categorizing, I set up another API call and had it choose a category from preexisting choices I put into the prompt. I then created a for loop, called the classification function, placed the response into a string, and used conditional statements with the \"in\" keyword to choose a dictionary category. It then appends the prompt to the list and dumps the results back into the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.26.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "arr = finalPassages.split(\"####\")\n",
    "\n",
    "def classifyPassages(cinput):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "        {\"role\": \"user\", \"content\": cinput}],\n",
    "        max_tokens=100,\n",
    "        temperature=0,\n",
    "    )\n",
    "    passage = response['choices'][0]['message']['content']\n",
    "    return passage\n",
    "\n",
    "with open(\"promptdatabase.json\", \"r\") as rf:\n",
    "    dict = json.load(rf)\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    pintro = 'QUERY: Classify this passage as one of the following - Culture, Astronomy, School, Food, Sports, or Other' + arr[i]\n",
    "    classification = classifyPassages(pintro)\n",
    "    if 'Culture' in classification:\n",
    "        dict[\"Culture\"].append(arr[i])\n",
    "    elif 'Astronomy' in classification:\n",
    "        dict[\"Astronomy\"].append(arr[i])\n",
    "    elif 'School' in classification:\n",
    "        dict[\"School\"].append(arr[i])\n",
    "    elif 'Food' in classification:\n",
    "        dict[\"Food\"].append(arr[i])\n",
    "    elif 'Sports' in classification:\n",
    "        dict[\"Sports\"].append(arr[i])\n",
    "    else:\n",
    "        dict[\"Other\"].append(arr[i])\n",
    "\n",
    "with open(\"promptdatabase.json\", \"w\") as wf:\n",
    "    json.dump(dict, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another script was also written that takes the input from a smaller paragraph of text and generates passages from them. This was essentially just changing the prompt, but I set it up to read from a txt file for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 07.26.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "import openai\n",
    "\n",
    "APIKEY = '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'\n",
    "openai.api_key = APIKEY\n",
    "\n",
    "initInstruction = 'You are a writer tasked with creating 5 120-word passages based on a paragraph of source material. You may draw from other sources in this writing.'\n",
    "\n",
    "with open('paragraphmaterial.txt', 'r') as f:\n",
    "    cinput = f.read()\n",
    "\n",
    "cinput = 'QUERY: Write 5 unique 120-word passages based loosely on this paragraph: ' + cinput\n",
    "\n",
    "def returnPrompts(sysinput, cinput):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": sysinput},\n",
    "        {\"role\": \"user\", \"content\": cinput}],\n",
    "        max_tokens=1000,\n",
    "        temperature=0.9,\n",
    "        presence_penalty = 0.8\n",
    "    )\n",
    "    reply = response['choices'][0]['message']['content']\n",
    "    return reply\n",
    "\n",
    "passages = returnPrompts(initInstruction, cinput)\n",
    "\n",
    "with open('paragenresponse.txt', 'a') as f:\n",
    "    f.write('\\n\\nSYSTEM PROMPT: ' + initInstruction + '\\n\\n')\n",
    "    f.write(cinput + '\\n\\n')\n",
    "    f.write('TEXT INPUT: ' + passages + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, two alternate ways of developing passages were created. Now, it was important to take both of these and combine them to form a script that optimally generated passages that was also easy to understand sequentially. Multiple edits were made and a completely new script was created. The API Key was moved to an environmental variable, the translation was moved over to the modification rather than happening initially, and the categorization and storing into the database was done within the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeb Lakey, 08.04.23, Model in the Mirror w/ Dr. Richard Medina\n",
    "basePassages = [\"Passage A\", \"Passage B\"]\n",
    "basecinput = \"develop a prompt blah blah\"\n",
    "for i in range(len(basePassages)):\n",
    "    finalPassages.append(simplifyReplies(userInput,(basecinput + basePassages[i])))\n",
    "    finalPassages[i] = ''.join(finalPassages.splitlines())\n",
    "    print(finalPassages[i])\n",
    "    pintro = 'QUERY: Classify this passage in english as one of the following - Culture, Astronomy, School, Food, Sports, or Other. Give a one word response. Pick ONLY from these categories.' + finalPassages[i]\n",
    "    classification = classifyPassages(pintro)\n",
    "    print(classification)\n",
    "    if 'Culture' in classification:\n",
    "        dict[\"Culture\"].append(finalPassages[i])\n",
    "    elif 'Astronomy' in classification:\n",
    "        dict[\"Astronomy\"].append(finalPassages[i])\n",
    "    elif 'School' in classification:\n",
    "        dict[\"School\"].append(finalPassages[i])\n",
    "    elif 'Food' in classification:\n",
    "        dict[\"Food\"].append(finalPassages[i])\n",
    "    elif 'Sports' in classification:\n",
    "        dict[\"Sports\"].append(finalPassages[i])\n",
    "    else:\n",
    "        dict[\"Other\"].append(finalPassages[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we felt as though it was best to move on to the c-test generation. These tests were done in english using the NLTK, which is a language processing platform for Python. After some initial hiccups getting everything set up, a very early iteration of the C-test was developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"We need to learn more about intersectionality to really understand it. We can't just have simple ideas about what it means. We need to look at its history and think about it carefully. By doing this, we can see how important intersectionality is and how it can help us make things better.\"\n",
    "lemm = nltk.WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "ctestfinal = \"\"\n",
    "\n",
    "def convert(input):\n",
    "    base = \"\"\n",
    "    print(str(input) + '\\n')\n",
    "    for i in range(len(input)):\n",
    "        if (i == 0):\n",
    "            base += (' ' + input[i] + ' ')\n",
    "        elif (input[i] != input[-1]) and (\"'\" in input[i+1]):\n",
    "            base += input[i]\n",
    "        elif (input[i] != input[-1]) and (',' in input[i+1]):\n",
    "            base += input[i]\n",
    "        elif (input[i] != input[-1]) and ('.' in input[i+1]):\n",
    "            base += input[i]\n",
    "        elif (input[i] != input[-1]):\n",
    "            base += (input[i] + ' ')\n",
    "        else:\n",
    "            base += input[i]\n",
    "    return base\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words[1] = lemm.lemmatize(words[1])\n",
    "    newword, trash = divmod(len(words[1]), 2)\n",
    "    words[1] = words[1][:newword + trash]\n",
    "    words[1] += (\"_\" * newword)\n",
    "    sentences[i] = words\n",
    "    conversion = convert(sentences[i])\n",
    "    ctestfinal += conversion\n",
    "\n",
    "ctestfinal = ctestfinal.lstrip(\" \")\n",
    "print(ctestfinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet uses NLTK to separate the string into sentences, and then further separates each sentence into a list of words. The second word of each list is modified using divmod, and is split in half. If the word has an odd number of letters, more letters are kept than taken away. The convert method at the top is to reconstruct the string for printing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groundwork was generally set up, but there was a problem that was encountered fairly quickly. NLTK did not support many of the languages that we wanted to attempt to create C-tests for, namely Korean and Chinese. We pivoted over to spaCy, which has separate models for different languages that you download yourself. This is example code from the Korean C-test generator that was developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ko_core_news_lg')\n",
    "\n",
    "doc = nlp(u\"\\uc218\\ud559 \\uc804\\ub7b5\\uc744 \\ub9d0\\ub85c \\uc124\\uba85\\ud558\\ub294 \\uac83\\ub9cc\\uc73c\\ub85c\\ub294 \\ud559\\uc0dd\\ub4e4\\uc774 \\uac1c\\ub150\\uc744 \\uc644\\uc804\\ud788 \\uc774\\ud574\\ud558\\uae30\\uc5d0\\ub294 \\ubd80\\uc871\\ud569\\ub2c8\\ub2e4. Kling\\uacfc Bay-Williams \\uad50\\uc721\\uc790\\ub4e4\\uc740 \\ud559\\uc0dd\\ub4e4\\uc774 \\uc22b\\uc790 \\uad00\\uacc4\\ub97c \\uc774\\ud574\\ud558\\uace0 \\uc2dc\\uac01\\ud654\\ud558\\uae30 \\uc704\\ud574 \\ucda9\\ubd84\\ud55c \\uc2dc\\uac04\\uacfc \\uacbd\\ud5d8\\uc774 \\ud544\\uc694\\ud558\\ub2e4\\uace0 \\uac15\\uc870\\ud569\\ub2c8\\ub2e4. \\ud559\\uc0dd\\ub4e4\\uc5d0\\uac8c \\uc774\\ud574\\ud558\\ub77c\\uace0 \\ub9d0\\ud558\\ub294 \\uac83\\ub9cc\\uc73c\\ub85c\\ub294 \\ucda9\\ubd84\\ud558\\uc9c0 \\uc54a\\uc73c\\uba70, \\uc218\\ud559\\uc744 \\uc2dc\\uac01\\uc801\\uc73c\\ub85c \\ub2e4\\ub8f0 \\uc218 \\uc788\\ub294 \\uae30\\ud68c\\uac00 \\ud544\\uc694\\ud569\\ub2c8\\ub2e4.\\n\\n\\ud558\\ub098\\uc758 \\ud765\\ubbf8\\ub85c\\uc6b4 \\uc2dc\\uac01\\uc801 \\uc804\\ub7b5\\uc740 Quick Look \\uce74\\ub4dc\\uc758 \\uc0ac\\uc6a9\\uc785\\ub2c8\\ub2e4. \\uc774 \\ud65c\\ub3d9\\uc740 \\ud559\\uc0dd\\ub4e4\\uc5d0\\uac8c \\uc810\\ub4e4\\uc758 \\uadf8\\ub8f9\\uc774\\ub098 \\uc775\\uc219\\ud55c \\ubb3c\\uac74\\ub4e4\\uc758 \\uc0ac\\uc9c4\\uc744 \\uc7a0\\uae50 \\ubcf4\\uc5ec\\uc8fc\\uace0, \\uadf8 \\uc218\\ub7c9\\uacfc \\ubc30\\uc5f4\\uc744 \\uacb0\\uc815\\ud558\\ub3c4\\ub85d \\uc720\\ub3c4\\ud558\\ub294 \\uac83\\uc785\\ub2c8\\ub2e4. \\uc810\\uc774\\ub098 \\uc870\\uc791 \\ub3c4\\uad6c\\ub97c \\ud1b5\\ud574 \\uc218\\ub97c \\uc2dc\\uac01\\uc801\\uc73c\\ub85c \\uc81c\\uc2dc\\ud568\\uc73c\\ub85c\\uc368, \\ud559\\uc0dd\\ub4e4\\uc740 \\uc218\\ud559 \\uc804\\ub7b5\\uc758 \\uadfc\\ubcf8\\uc801\\uc778 \\uc6d0\\ub9ac\\uc5d0 \\ub300\\ud55c \\uae4a\\uc740 \\uc774\\ud574\\ub97c \\ubc1c\\uc804\\uc2dc\\ud0ac \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\ub610 \\ub2e4\\ub978 \\ud6a8\\uacfc\\uc801\\uc778 \\uc2dc\\uac01 \\ub3c4\\uad6c\\ub294 \\uc218\\ud559 \\ucf54\\uce58 Steve Wyborney\\uac00 \\ub9cc\\ub4e0 Splat! \\uac8c\\uc784\\uc785\\ub2c8\\ub2e4. \\uc2ac\\ub77c\\uc774\\ub4dc\\ub4e4\\uc744 \\ud1b5\\ud574 \\ud559\\uc0dd\\ub4e4\\uc5d0\\uac8c \\uc810\\uc758 \\uc218\\ub97c \\uc138\\uace0, \\uadf8 \\uc911 \\uba87 \\uac1c\\uac00 splat\\uc5d0 \\uc758\\ud574 \\uac00\\ub824\\uc84c\\ub294\\uc9c0\\ub97c \\uc2dd\\ubcc4\\ud558\\ub3c4\\ub85d \\uc694\\uccad\\ud569\\ub2c8\\ub2e4. \\uc774 \\ud65c\\ub3d9\\uc740 \\ud559\\uc0dd\\ub4e4\\uc774 \\uc790\\uc2e0\\ub9cc\\uc758 \\uc804\\ub7b5\\uc744 \\uac1c\\ubc1c\\ud558\\ub3c4\\ub85d \\uc720\\ub3c4\\ud558\\uba70, \\uc608\\ub97c \\ub4e4\\uc5b4 \\uc18c\\ub9ac\\ub098 \\uc190\\uac00\\ub77d\\uc744 \\uc0ac\\uc6a9\\ud558\\uc5ec \\uc704\\ub85c \\ub610\\ub294 \\uc544\\ub798\\ub85c \\uc138\\ub294 \\ub4f1\\uc785\\ub2c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\uc2dc\\uac01\\uc801\\uc778 \\uc5f0\\uc2b5\\uc5d0 \\uc801\\uadf9\\uc801\\uc73c\\ub85c \\ucc38\\uc5ec\\ud568\\uc73c\\ub85c\\uc368, \\ud559\\uc0dd\\ub4e4\\uc740 \\ubb38\\uc81c \\ud574\\uacb0 \\ub2a5\\ub825\\uc744 \\ud5a5\\uc0c1\\uc2dc\\ud0a4\\uace0 \\uc218\\ud559\\uc801 \\uac1c\\ub150\\uc744 \\ub354 \\uac15\\ud558\\uac8c \\uc774\\ud574\\ud560 \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\\n\\n\\uc218\\ud559 \\uc5f0\\uc0b0\\uc758 \\ud3ec\\uad04\\uc801\\uc778 \\uc774\\ud574\\ub97c \\ubcf4\\uc7a5\\ud558\\uae30 \\uc704\\ud574\\uc11c\\ub294, \\ub367\\uc148\\uacfc \\uacf1\\uc148 \\uc0ac\\uc2e4\\uc744 \\ub354\\ud558\\ub294 \\uc218\\ub098 \\uacf1\\ud558\\ub294 \\uc218\\uc758 \\ud06c\\uae30\\uc5d0\\ub9cc \\uc758\\uc874\\ud558\\uc5ec \\uac00\\ub974\\uce58\\ub294 \\uac83\\uc744 \\ud53c\\ud574\\uc57c \\ud569\\ub2c8\\ub2e4. Kling\\uacfc Bay-Williams\\ub294 2, 10, 5\\uc640 \\uac19\\uc740 \\uae30\\ucd08\\uc801\\uc778 \\uc9d1\\ud569\\ubd80\\ud130 \\uc2dc\\uc791\\ud558\\ub294 \\uac83\\uc774 \\ub354 \\ud6a8\\uacfc\\uc801\\uc774\\ub77c\\uace0 \\uc8fc\\uc7a5\\ud569\\ub2c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\uc9d1\\ud569\\uc740 \\uc775\\uc219\\ud568\\uc744 \\uc81c\\uacf5\\ud560 \\ubfd0\\ub9cc \\uc544\\ub2c8\\ub77c \\ub2e4\\ub978 \\uc218\\ud559 \\uc0ac\\uc2e4\\uc744 \\uc720\\ub3c4\\ud558\\ub294 \\uae30\\ucd08\\ub85c\\ub3c4 \\uc791\\uc6a9\\ud569\\ub2c8\\ub2e4. \\uc774\\ub7ec\\ud55c \\uae30\\ucd08\\uc801\\uc778 \\uc9d1\\ud569\\uc744 \\uc219\\ub2ec\\ud568\\uc73c\\ub85c\\uc368, \\ud559\\uc0dd\\ub4e4\\uc740 \\ubcf5\\uc7a1\\ud55c \\ubb38\\uc81c\\ub97c \\ub354 \\uac04\\ub2e8\\ud55c \\uad6c\\uc131 \\uc694\\uc18c\\ub85c \\ubd84\\ud574\\ud558\\uc5ec \\uc811\\uadfc\\ud560 \\uc218 \\uc788\\uc73c\\uba70, \\uc218\\ud559\\uc801 \\uacc4\\uc0b0\\uc5d0\\uc11c \\ub354 \\ud070 \\uc131\\uacf5\\uc744 \\uc774\\ub04c\\uc5b4 \\ub0bc \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "base = \"\"\n",
    "sentencecounter = 1\n",
    "#every word w token.pos >2 gap astrix before+after gap\n",
    "for sent in doc.sents:\n",
    "    counter = 1\n",
    "    nounFound = False\n",
    "    for token in sent:\n",
    "        if counter%2 == 0 and nounFound is False and len(token.shape_) > 1 and sentencecounter != 1:\n",
    "            ttoken = str(token)\n",
    "            newword, trash = divmod(len(ttoken), 2)\n",
    "            ttoken = \" \" + ttoken[:newword] + \"*\" + ttoken[newword:] + \"*\"\n",
    "            #nounFound = True\n",
    "        elif token.pos_ == 'PUNCT':\n",
    "            ttoken = str(token)\n",
    "        else:\n",
    "            ttoken = \" \" + str(token)\n",
    "        counter = counter + 1\n",
    "        base = base + ttoken\n",
    "    sentencecounter = sentencecounter + 1\n",
    "        \n",
    "base = base.lstrip(' ')\n",
    "print(doc.text + '\\n\\n')\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, spacy is imported and the nlp loaded is ko-core-news-lg, which has a higher size but lower efficiency. The string is converted into spacy's generator object, which is then split into sentences and and then further into tokens. Essentially, the program's current output is that every second word within the string is split in half, and then the second half of the word is surrounded by astrixes. The other conditions are to handle proper spacing. The words are also only gapped if length is greater than one. The nounFound boolean is old code in a prior test where we wanted to specify only nouns for gapping (which is useful in its own right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other database files and spacy c-test generators were created for other non-English languages. This included chinese and portuguese tests. Overall reactions to the early C-tests generated were positive, but requested that proper nouns be left out of the gapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "import spacyfunctions as sf\n",
    "genres = []\n",
    "difficulty = []\n",
    "lang = 'sample'\n",
    "num = int(input('Please select an option: \\nINPUT 1: Random C-test\\nINPUT 2: Random C-Test from selected genre\\nINPUT 3: All passages within a selected difficulty and genre converted to C-tests in input language\\nINPUT 4: All passages in all genres within a selected difficulty converted to C-tests in input language\\nINPUT 5: All passages converted to C-tests in input language\\n'))\n",
    "num2 = int(input('INPUT 1: Terminal\\nINPUT 2: .txt\\nINPUT 3: h5p content'))\n",
    "if num == 1:\n",
    "    while True: \n",
    "        gIndex = randint(0, len(genres)-1)\n",
    "        genre = genres[gIndex]\n",
    "        if len(dict[difficulty][genre]) > 0:\n",
    "            break\n",
    "    index = randint(0, len(dict[difficulty][genre])-1) \n",
    "    passage = dict[difficulty][genre][index]\n",
    "    if num2 == 1:\n",
    "        sf.printAllTypes(passage, lang)\n",
    "    elif num2 == 2:\n",
    "        t = time.localtime()\n",
    "        mod = f'{str(t.tm_year)}-{str(t.tm_mon)}-{str(t.tm_mday)}_{str(t.tm_hour)}-{str(t.tm_min)}'\n",
    "        file = open(f'./testfiles/ctest{mod}', 'w')\n",
    "        sf.writeAllTypes(passage, lang, file)\n",
    "    elif num2 == 3:\n",
    "        t = time.localtime()\n",
    "        mod = f'{str(t.tm_year)}-{str(t.tm_mon)}-{str(t.tm_mday)}_{str(t.tm_hour)}-{str(t.tm_min)}'\n",
    "        with open ('./h5pdev/content-template.json', 'r') as f:\n",
    "            d = json.loads(f.read())\n",
    "        d[\"questions\"] = passage\n",
    "        with open(f'./h5pdev/{mod}/content.json', \"w\") as wf:\n",
    "            json.dump(d, wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "We found that although the passage creation process was relatively streamlined, it was important to also create a functional way to create C-tests from the passages stored within the databases. The code above is a snippet of that. The user can select from a series of options. The one above selects a random C-test from the database and prints the passage, C-test, and C-test with parts of speech to the terminal, file, or h5p content file. The actual printing process is done within a function stored in the new spacyfunctions.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacyfunctions import testGen\n",
    "#Print all tests to terminal\n",
    "def printAllTypes(passage, lang):\n",
    "    ctestPos = testGen(passage, lang, True)\n",
    "    ctestnPos = testGen(passage, lang, False)\n",
    "    print(f'Original Passage:\\n{passage}\\n')\n",
    "    print(f'C-Test (No Parts of Speech):\\n{ctestnPos}\\n')\n",
    "    print(f'C-Test (Parts of Speech):\\n{ctestPos}')\n",
    "\n",
    "#Print all tests to file\n",
    "def writeAllTypes(passage, lang, file):\n",
    "    ctestPos = testGen(passage, lang, True)\n",
    "    ctestnPos = testGen(passage, lang, False)\n",
    "    file.write(f'Original Passage:\\n{passage}\\n')\n",
    "    file.write(f'C-Test (No Parts of Speech):\\n{ctestnPos}\\n')\n",
    "    file.write(f'C-Test (Parts of Speech):\\n{ctestPos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the output (italics are what would be gapped):\n",
    "Original Passage:\n",
    "O complexo industrial das abelhas Ã© uma parte intricada e fascinante do nosso mundo, com uma histÃ³ria que abrange sÃ©culos. Os seres humanos hÃ¡ muito reconhecem o valor das abelhas e suas incrÃ­veis habilidades. CivilizaÃ§Ãµes antigas como a SumÃ©ria e a BabilÃ´nia mencionaram as abelhas em suas escritas cuneiformes, destacando sua importÃ¢ncia naquela Ã©poca. AtÃ© mesmo os faraÃ³s egÃ­pcios entendiam a importÃ¢ncia das abelhas, tendo-as domesticado jÃ¡ em 2400 a.C. Naturalistas romanos documentaram os mÃ©todos engenhosos empregados pelos apicultores no norte da ItÃ¡lia, que transportavam suas colmeias em navios para acessar novos campos de flores. A conexÃ£o entre as abelhas e os seres humanos estÃ¡ profundamente enraizada em nosso passado e continua a moldar nosso presente. Hoje, os apicultores migratÃ³rios comerciais formam uma parte essencial do nosso sistema agrÃ­cola, garantindo a polinizaÃ§Ã£o de culturas vitais. No entanto, a queda na populaÃ§Ã£o de abelhas representa uma ameaÃ§a significativa para essa rede intricada. A sobrevivÃªncia das abelhas nÃ£o Ã© apenas uma questÃ£o de sustentar sua espÃ©cie; Ã© crucial para a sustentabilidade de nossa produÃ§Ã£o de alimentos e o delicado equilÃ­brio de nosso ecossistema.\n",
    "\n",
    "C-Test (No Parts of Speech):\n",
    "O complexo industrial das abelhas Ã© uma parte intricada e fascinante do nosso mundo, com uma histÃ³ria que abrange sÃ©culos. Os se*res* humanos h*Ã¡* muito recon*hecem* o va*lor* das abe*lhas* e su*as* incrÃ­veis habil*idades*. CivilizaÃ§Ãµes ant*igas* como a SumÃ©ria e a BabilÃ´nia mencionaram a*s* abelhas e*m* suas escr*itas* cuneiformes, destacando s*ua* importÃ¢ncia naq*uela* Ã©poca. AtÃ© me*smo* os far*aÃ³s* egÃ­pcios ente*ndiam* a impor*tÃ¢ncia* das abe*lhas*, tend*o-as* domesticado j*Ã¡* em 2400 a. C. Naturalistas rom*anos* documentaram o*s* mÃ©todos engen*hosos* empregados pe*los* apicultores n*o* norte d*a* ItÃ¡lia, que transp*ortavam* suas colm*eias* em nav*ios* para ace*ssar* novos cam*pos* de flo*res*. A con*exÃ£o* entre a*s* abelhas e os se*res* humanos es*tÃ¡* profundamente enra*izada* em no*sso* passado e continua a moldar no*sso* presente. Hoje, os apicu*ltores* migratÃ³rios comer*ciais* formam u*ma* parte esse*ncial* do no*sso* sistema agrÃ­*cola*, garan*tindo* a polin*izaÃ§Ã£o* de cult*uras* vitais. No ent*anto*, a queda n*a* populaÃ§Ã£o d*e* abelhas repre*senta* uma ame*aÃ§a* significativa pa*ra* essa re*de* intricada. A sobrev*ivÃªncia* das abe*lhas* nÃ£o Ã© apenas uma questÃ£o d*e* sustentar s*ua* espÃ©cie; Ã© cru*cial* para a sustentabilidade d*e* nossa prod*uÃ§Ã£o* de alim*entos* e o delicado equil*Ã­brio* de no*sso* ecossistema.\n",
    "\n",
    "C-Test (Parts of Speech):\n",
    "O complexo industrial das abelhas Ã© uma parte intricada e fascinante do nosso mundo, com uma histÃ³ria que abrange sÃ©culos. Os se*res*(NOUN) humanos h*Ã¡*(VERB) muito recon*hecem*(VERB) o va*lor*(NOUN) das abe*lhas*(NOUN) e su*as*(DET) incrÃ­veis habil*idades*(NOUN). CivilizaÃ§Ãµes ant*igas*(ADJ) como a SumÃ©ria e a BabilÃ´nia mencionaram a*s*(DET) abelhas e*m*(ADP) suas escr*itas*(VERB) cuneiformes, destacando s*ua*(DET) importÃ¢ncia naq*uela*(ADP) Ã©poca. AtÃ© me*smo*(ADV) os far*aÃ³s*(NOUN) egÃ­pcios ente*ndiam*(VERB) a impor*tÃ¢ncia*(NOUN) das abe*lhas*(NOUN), tend*o-as*(ADJ) domesticado j*Ã¡*(ADV) em 2400 a. C. Naturalistas rom*anos*(NOUN) documentaram o*s*(DET) mÃ©todos engen*hosos*(ADJ) empregados pe*los*(ADP) apicultores n*o*(ADP) norte d*a*(ADP) ItÃ¡lia, que transp*ortavam*(VERB) suas colm*eias*(NOUN) em nav*ios*(NOUN) para ace*ssar*(VERB) novos cam*pos*(NOUN) de flo*res*(NOUN). A con*exÃ£o*(NOUN) entre a*s*(DET) abelhas e os se*res*(NOUN) humanos es*tÃ¡*(AUX) profundamente enra*izada*(VERB) em no*sso*(DET) passado e continua a moldar no*sso*(DET) presente. Hoje, os apicu*ltores*(NOUN) migratÃ³rios comer*ciais*(ADJ) formam u*ma*(DET) parte esse*ncial*(ADJ) do no*sso*(DET) sistema agrÃ­*cola*(ADJ), garan*tindo*(VERB) a polin*izaÃ§Ã£o*(NOUN) de cult*uras*(NOUN) vitais. No ent*anto*(NOUN), a queda n*a*(ADP) populaÃ§Ã£o d*e*(ADP) abelhas repre*senta*(VERB) uma ame*aÃ§a*(NOUN) significativa pa*ra*(ADP) essa re*de*(NOUN) intricada. A sobrev*ivÃªncia*(NOUN) das abe*lhas*(NOUN) nÃ£o Ã© apenas uma questÃ£o d*e*(SCONJ) sustentar s*ua*(DET) espÃ©cie; Ã© cru*cial*(ADJ) para a sustentabilidade d*e*(ADP) nossa prod*uÃ§Ã£o*(NOUN) de alim*entos*(NOUN) e o delicado equil*Ã­brio*(NOUN) de no*sso*(DET) ecossistema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the sections of code that print. Overall, simple. testGen is the new primary C-test conversion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testGen(passage, language, posIncluded=False, lenExclude=1, freq=2, posExclude=['PROPN', 'NUM', 'X', 'SPACE']):\n",
    "    return passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen here, testGen contains many parameters for the user to customize the C-test to their liking. Code will be coming in the future so these can be modified by the user. As a brief overview:\n",
    "<ul>\n",
    "<li>Passage - the passage</li>\n",
    "<li>Language - the language</li>\n",
    "<li>posIncluded - whether parts of speech are included. default is false</li>\n",
    "<li>lenExclude - any token length under this value will be excluded from gapping</li>\n",
    "<li>freq - how often words are being gapped</li>\n",
    "<li>posExclude - parts of speech that are excluded from gapping</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
